{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# II. PREDICT AVERAGE SESSION LENGTH\n",
    "########################################################################\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.model_selection import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to find IP address class\n",
    "def find_class(ip):\n",
    "    a=int(ip.split('.')[0])\n",
    "    \n",
    "  \n",
    "    # Class A \n",
    "    if (a >=1 and a <= 126) :\n",
    "        return 1 \n",
    "  \n",
    "    # Class B \n",
    "    elif (a >= 128 and a <= 191): \n",
    "        return 2\n",
    "  \n",
    "    # Class C \n",
    "    elif (a >= 192 and a <= 223) :\n",
    "        return 3 \n",
    "  \n",
    "    # Class D \n",
    "    elif (a >= 224 and a <= 239) :\n",
    "        return 4 \n",
    "    # Class E\n",
    "    else:\n",
    "        return 5 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read output of URL hits from DE section\n",
    "\n",
    "hits = pd.read_csv('/Users/bjayaram/Downloads/WeblogChallenge-master/data/output/url_hits/part-00000-d78ec039-454d-42c2-908a-bf9fd79a2248-c000.csv',low_memory=False)\n",
    "print hits.columns\n",
    "\n",
    "data = hits[['session_id','ip','unique_hits']].groupby(['session_id','ip'],as_index=False).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split IP address into 4 features\n",
    "X = data['ip'].str.split('.', expand=True)\n",
    "for i in range(4):\n",
    "    c = chr(ord('a') + i)\n",
    "    X[c] = map(int, X[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add session id\n",
    "X['session_id'] = data['session_id']\n",
    "\n",
    "# find class (i.e A, B, C, D) for each IP address. Method find_class() defined in parse_ip.py\n",
    "X['class'] = map(find_class, data['ip'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extracted (276932, 6) (276932,)\n"
     ]
    }
   ],
   "source": [
    "#  X contains IP class, session id, 4 components of IP address\n",
    "X = X[['class', 'session_id', 'a', 'b', 'c', 'd']].values\n",
    "\n",
    "# Y contains number of unique URL hits\n",
    "Y = data['unique_hits'].values\n",
    "print \"Data extracted\", X.shape, Y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = range(10,10000,10)\n",
    "alphas = map(lambda x: 10.0/(1+x), alpha_list)\n",
    "results={}\n",
    "\n",
    "\n",
    "# Try the following regressors: \n",
    "# 1.Linear Regression, \n",
    "# 2.GradientBoostingRegressor, \n",
    "# 3.Ridge, \n",
    "# 4.Lasso, \n",
    "# 5.RandomForest\n",
    "model1 = GradientBoostingRegressor(n_estimators=1000)\n",
    "model2 = RidgeCV(normalize=True, alphas = alphas)\n",
    "model3 = LassoCV(normalize=True, alphas = alphas, max_iter=1000)\n",
    "model4 = RandomForestRegressor(n_estimators=1000)\n",
    "model5 = LinearRegression(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the following regressors: Linear Regression, GradientBoostingRegressor, Ridge, Lasso, RandomForest\n",
    "model1 = GradientBoostingRegressor(n_estimators=100)\n",
    "model2 = RidgeCV(normalize=True, alphas = alphas)\n",
    "model3 = LassoCV(normalize=True, alphas = alphas, max_iter=1000)\n",
    "model4 = RandomForestRegressor(n_estimators=1000)\n",
    "model5 = LinearRegression(normalize=True)\n",
    "\n",
    "\n",
    "# Run 10-fold cross validation on each of the models to evaluate the performance\n",
    "print \"Evaluating Gradient Boosting regressor\"\n",
    "results1 = cross_val_score(model1, X, Y, cv=10, n_jobs=-1,scoring='neg_mean_squared_error')\n",
    "\n",
    "print \"Evaluating Ridge regressor\"\n",
    "results2 = cross_val_score(model2, X, Y, cv=10, n_jobs=-1,scoring='neg_mean_squared_error')\n",
    "\n",
    "print \"Evaluating Lasso regressor\"\n",
    "results3 = cross_val_score(model3, X, Y, cv=10, n_jobs=-1,scoring='neg_mean_squared_error')\n",
    "\n",
    "print \"Evaluating Random Forest regressor\"\n",
    "results4 = cross_val_score(model4, X, Y, cv=10, n_jobs=-1,scoring='neg_mean_squared_error')\n",
    "\n",
    "print \"Evaluating Linear regressor\"\n",
    "results5 = cross_val_score(model5, X, Y, cv=10, n_jobs=-1,scoring='neg_mean_squared_error')\n",
    "\n",
    "\n",
    "results['Gradient Boosting Regression']=results1.mean()\n",
    "results['Ridge Regresison']=results2.mean()\n",
    "results['Lasso Regression']=results3.mean()\n",
    "results['RandomForestRegressor']=results4.mean()\n",
    "results['Linear Regression']=results5.mean()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print results\n",
    "# -ve MSE (in minutes) values are as follows:\n",
    "# 'Ridge Regresison': -30.641001729376807,\n",
    "# 'Lasso Regression': -30.608556949805347,\n",
    "# 'Gradient Boosting Regression': -30.054393398542505,\n",
    "# 'RandomForestRegressor': -32.60778837997033}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further iterate with Gradient Boosting Regressor\n",
    "\n",
    "\n",
    "lr = map(lambda x:10 ** -x, range(1,4))\n",
    "\n",
    "model=GridSearchCV(GradientBoostingRegressor(),n_jobs=-1, cv=10,scoring='neg_mean_squared_error',verbose=1,\n",
    "                   param_grid={'learning_rate':lr,'n_estimators':[100,1000,10000]})\n",
    "model.fit(X,Y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print \"BEST SCORE\", model.best_score_, model.scoring\n",
    "print \"BEST ESTIMATOR\", model.best_estimator_\n",
    "pickle.dump(model,open('/Users/bjayaram/Downloads/WeblogChallenge-master/WebLogsPrediction/output/intervals_model.p','w'))\n",
    "# BEST SCORE: 29.9875736945 hits neg_mean_squared_error\n",
    "# BEST ESTIMATOR:\n",
    "# GridSearchCV(cv=10, error_score='raise',\n",
    "#        estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
    "#              learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
    "#              max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "#              min_impurity_split=None, min_samples_leaf=1,\n",
    "#              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "#              n_estimators=100, presort='auto', random_state=None,\n",
    "#              subsample=1.0, verbose=0, warm_start=False),\n",
    "#        fit_params=None, iid=True, n_jobs=-1,\n",
    "#        param_grid={'n_estimators': [100], 'learning_rate': [0.1, 0.01, 0.001]},\n",
    "#        pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
    "#        scoring='neg_mean_squared_error', verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
